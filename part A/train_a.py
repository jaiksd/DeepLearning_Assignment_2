# -*- coding: utf-8 -*-
"""Train_part_A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Do9wVkxtKMLqHBbKIxogvCyAXMSSV1S6

Deep Learning Assignment 2 part A
CS23M030
"""
#comments are added in every line
#importing all the necessary pytorch libraries
import torchvision  # Import torchvision library for computer vision tasks
import torch  # Import PyTorch library
import torch.nn as nn  # Import neural network module from PyTorch
import seaborn as sns  # Import seaborn for data visualization
sns.set()  # Set seaborn settings
from torch.utils.data import DataLoader  # Import DataLoader for handling datasets in PyTorch
import torchvision.datasets as datasets  # Import datasets module from torchvision
import torch.optim as optim  # Import optimization module from PyTorch

import matplotlib.pyplot as plt  # Import matplotlib for plotting
import torchvision.models as models  # Import pretrained models from torchvision
import argparse  # Import argparse for parsing command-line arguments
import warnings  # Import warnings module
warnings.filterwarnings("ignore")  # Ignore warnings
import torchvision.transforms as transforms  # Import transforms module from torchvision
from torch.utils.data import random_split  # Import random_split for splitting datasets
import numpy as np  # Import numpy for numerical computations
# !pip install wandb  # Install wandb library
import wandb  # Import wandb for experiment tracking



#Please install all the libraries i have imported before running 
#then upload the dataset in the current directory or change the path of trainset and testset according to dataset location

def data_loading():
    trainset= "/content/inaturalist_12K/train"
    testset = "/content/inaturalist_12K/val"
    return trainset,testset

"""Q1.Building my CNN model"""

"""
This module defines a custom Convolutional Neural Network (CNN) model for image classification.
    Arguments
    num_filter: A list specifying the number of filters for each convolutional layer.
    kernel_dimensions: A list specifying the kernel size (filter size) for each convolutional layer.
    conv_activation: Activation function used in convolutional layers. Default is 'mish'.
    use_batch_normalization: Boolean indicating whether to use batch normalization. Default is True.
    dropout_probability: dropout_probability probability for dropout_probability layers. Default is 0.1.
    hidden_layer_size: Size of the fully connected layer. Default is 256.
    fully_connected_activation: Activation function used in the fully connected layer. Default is 'mish'.
    strideLength: Stride size for convolutional layers. Default is 2.
    img_len: Initial length of the input image. Default is 224.
    img_wid: Initial width of the input image. Default is 224.

"""

class Deep_Image_Classifier(nn.Module):
    def __init__(self, num_filter=[64,64,64,64,64], kernel_dimensions=[3,3,3,3,3], conv_activation='mish',use_batch_normalization=True,dropout_probability=0.1,hidden_layer_size=256,fully_connected_activation='mish' ,strideLength=2,img_len=224,img_wid=224):
        super(Deep_Image_Classifier,self).__init__()

        # Initializing all the class members
        # Model hyperparameters
        self.wid = img_wid  # Width of the input image
        self.fully_connected_activation = fully_connected_activation  # Activation function for fully connected layers
        self.density = 1000  # A parameter, possibly related to the network architecture or task
        self.len = img_len  # Length of the input image

        x=0
        while x<5:
             #this len will contain the final length of the image
             cal_len=(kernel_dimensions[x] - 1)
             self.len = (self.len - cal_len) // strideLength
             #this wid will contain the final width of the image
             cal_wid=(kernel_dimensions[x] - 1)
             self.wid = (self.wid - cal_wid) // strideLength
             x+=1


        #defining the dictionary for activation function
        act_func = {'gelu':nn.GELU(),'relu': nn.ReLU(),'silu':nn.SiLU(),'mish':nn.Mish()}
        self.conv_activation=conv_activation# Activation function for convolutional layers
        self.use_batch_normalization=use_batch_normalization # Whether to use batch normalization
        self.hidden_layer_size=hidden_layer_size #Size of Fully connected layer
        self.dropout_probability=dropout_probability # dropout_probability probability

        self.strideLength = strideLength  # Stride size for convolutional layers

        n4 = num_filter[4]  # Number of filters for the fourth layer
        k4 = kernel_dimensions[4]  # Kernel dimensions for the fourth layer
        w_length = 10  # Length parameter, perhaps related to the task or dataset
        n0 = num_filter[0]  # Number of filters for the initial layer
        k0 = kernel_dimensions[0]  # Kernel dimensions for the initial layer

        self.layers = nn.ModuleList([
            # Defining the first layer
            nn.Conv2d(in_channels=3, out_channels=n0, kernel_size=k0),  # Convolutional layer
            act_func[self.conv_activation],  # Activation function for the convolutional layer
            nn.BatchNorm2d(n0) if self.use_batch_normalization == True else nn.Identity(),  # Batch normalization if enabled
            nn.MaxPool2d(kernel_size=2, stride=strideLength),  # Max pooling operation
        

            # Defining the second layer
            nn.Conv2d(num_filter[0], num_filter[1], kernel_dimensions[1]),  # Convolutional layer
            act_func[self.conv_activation],  # Activation function for the convolutional layer
            nn.BatchNorm2d(num_filter[1]) if self.use_batch_normalization == True else nn.Identity(),  # Batch normalization if enabled
            nn.MaxPool2d(kernel_size=2, stride=strideLength),  # Max pooling operation


            #Defining the third layer
            #All the convolutional layer contains a 2d conv layer followed by
            #A layer of activation function
            #Followed by a 2d maxpooling layer
           # Defining the third layer
            nn.Conv2d(num_filter[1], num_filter[2], kernel_dimensions[2]),  # Convolutional layer
            act_func[self.conv_activation],  # Activation function for the convolutional layer
            nn.BatchNorm2d(num_filter[2]) if self.use_batch_normalization == True else nn.Identity(),  # Batch normalization if enabled
            nn.MaxPool2d(kernel_size=2, stride=strideLength),  # Max pooling operation

            # Defining the fourth layer
            nn.Conv2d(num_filter[2], num_filter[3], kernel_dimensions[3]),  # Convolutional layer
            act_func[self.conv_activation],  # Activation function for the convolutional layer
            nn.BatchNorm2d(num_filter[3]) if self.use_batch_normalization == True else nn.Identity(),  # Batch normalization if enabled
            nn.MaxPool2d(kernel_size=2, stride=strideLength),  # Max pooling operation

            # Defining the fifth layer
            nn.Conv2d(num_filter[3], n4, k4),  # Convolutional layer
            act_func[self.conv_activation],  # Activation function for the convolutional layer
            nn.BatchNorm2d(n4) if self.use_batch_normalization == True else nn.Identity(),  # Batch normalization if enabled
            nn.MaxPool2d(kernel_size=2, stride=strideLength),  # Max pooling operation

            # Last fully connected layer
            nn.Flatten(),  # Flatten the output for fully connected layers
            nn.Linear(int(n4*self.len*self.wid), hidden_layer_size),  # Fully connected layer
            act_func[self.conv_activation],  # Activation function for the fully connected layer
            nn.Dropout(dropout_probability),  # Dropout layer for regularization
            nn.Linear(hidden_layer_size, w_length)  # Final fully connected layer

        ])
    def forwarpropagation(self,y):
        i=y
        return i

    # Defining the forward propagation function for the model
    def forward(self, x):
        i = x  # Assigning input to variable i
        for l in self.layers:  # Looping through the layers
            x = l(x)  # Applying each layer to the input
        return x  # Returning the output after passing through all layers

"""Checking the availability of GPU"""
# Below part of code checks for availability of GPU
torch.cuda.device_count()  # Check the number of available GPUs
check = torch.cuda.is_available()  # Check if GPU is available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # Set device to GPU if available, else to CPU
print(device)  # Print the device being used (GPU or CPU)

#calculating accuracy and loss
#for loss we are using cross entropy loss function
def calculateAccAndError(dataloader ,net,loss_fn ):
    """
      Calculates the accuracy and average loss for a given model on a dataset.

      Args:
          dataloader (torch.utils.data.DataLoader): Data loader for the dataset.
          net (torch.nn.Module): The neural network model.
          loss_fn (torch.nn.Module): The loss function to use.

      Returns:
          tuple: (accuracy, average_loss)
      """
    epoch_loss_array = []# Store loss values for each batch
    #setting the total and correct to 0 initially
    tot, correct_label = 0, 0

    # Iterate through the dataloader in batches
    for data in dataloader:
        inputs, labels = data
        # Move tensors to the configured device (CPU or GPU)
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = net(inputs) # Get output predictions from the model
        def calcerr(self,y):
            i=y
            return i
        # Calculate loss for this batch
        loss = loss_fn(outputs, labels)
        epoch_loss_array.append(loss.item())
        #we are using the index of the max value and not using the maximum value itself that's why we have used '_'

        # Get predicted class indices (ignoring actual output values)
        _, pred = torch.max(outputs.data, 1)
        #I have added functions for modularity in the code
        def calprediction(self,prediction):
            z=prediction
            z=do_pred()
            return z

        # Increment total samples and correct predictions based on comparison with labels
        tot += labels.size(0)
        correct_label += (pred == labels).sum().item()
    #returning the accuracy and loss
    return 100 * correct_label / tot,sum(epoch_loss_array)/len(epoch_loss_array)

#defining my main function and if the parameters are not passed then it will take default values
def main(num_filter=[64,64,64,64,64], kernel_dimensions=[3,3,3,3,3], conv_activation='mish',enable_data_aug=True,use_batch_normalization=True,dropout_probability=0.1,hidden_layer_size=256,fully_connected_activation='mish' ,strideLength=2,maximum_training_epochs=10):
    #loading the data into the trainset and testset
    trainset,testset=data_loading()
    #all the values are taken into variable for flexibily, so that you can change if you want to
    p = 0.5  # Probability for random horizontal flip
    rotate = 10  # Rotation angle for random rotation
    resize = 224  # Size for resizing
    mean1 = 0.485  # Mean value for normalization (channel 1)
    mean2 = 0.456  # Mean value for normalization (channel 2)
    mean3 = 0.406  # Mean value for normalization (channel 3)
    variance1 = 0.229  # Variance value for normalization (channel 1)
    variance2 = 0.224  # Variance value for normalization (channel 2)
    variance3 = 0.225  # Variance value for normalization (channel 3)
    scale1 = 0.02  # Minimum area scale for random erasing
    scale2 = 0.33  # Maximum area scale for random erasing
    ratio1 = 0.3  # Minimum aspect ratio for random erasing
    ratio2 = 3.3  # Maximum aspect ratio for random erasing

    # Applying transformations on the train data
    transform_train = transforms.Compose([
        transforms.RandomHorizontalFlip(p),  # Random horizontal flip
        transforms.RandomRotation(rotate),  # Random rotation
        transforms.Resize((resize, resize)),  # Resize to specified dimensions
        transforms.ToTensor(),  # Convert image to tensor
        transforms.Normalize((mean1, mean2, mean3), (variance1, variance2, variance3)),  # Normalize image
        transforms.CenterCrop((resize, resize)),  # Center crop to specified dimensions
        transforms.RandomErasing(p, scale=(scale1, scale2), ratio=(ratio1, ratio2)),  # Random erasing
    ])


    # Applying transformations on the test data
    transform = transforms.Compose([
        transforms.Resize((resize, resize)),  # Resize to specified dimensions
        transforms.ToTensor(),  # Convert image to tensor
        transforms.Normalize((mean1, mean2, mean3), (variance1, variance2, variance3))  # Normalize image
    ])



    # Define a function to extract and return train data
    def ret_train(trainset, transform_t):
        """
        This function is used to extract and return train data.

        Args:
        trainset (str): Path to the train set.
        transform_t (torchvision.transforms.Compose): Transformations to be applied to the train data.

        Returns:
        torchvision.datasets.ImageFolder: Train data with specified transformations.
        """
        return datasets.ImageFolder(trainset, transform_t)

    # Define a function to extract and return test data
    def ret_test(testset, test_transform):
        """
        This function is used to extract and return test data.

        Args:
        testset (str): Path to the test set.
        test_transform (torchvision.transforms.Compose): Transformations to be applied to the test data.

        Returns:
        torchvision.datasets.ImageFolder: Test data with specified transformations.
        """
        return datasets.ImageFolder(testset, test_transform)

    check = enable_data_aug  # Variable check stores whether data augmentation is enabled


    #Checking for the data augmentation
    if not check:
        # If data augmentation is not enabled, use regular transformations for both train and test sets
        trainset = ret_train(trainset, transform)
        testset = ret_test(testset, transform)
    elif check:
        # If data augmentation is enabled, use augmented transformations for both train and test sets
        trainset = ret_train(trainset, transform_train)
        testset = ret_test(testset, transform)


    
    calcu = len(trainset)  # Calculate the length of the train set
    calcu = int(np.floor(0.2 * calcu))  # Calculate 20% of the train set length for validation
    n_validation = calcu  # Set the number of validation samples

    #splitting the training data into trainset and the validation set
    def do_split(trainset, n_training, n_validation):
        """
        Function to split the dataset into training and validation sets.

        Args:
        trainset (torch.utils.data.Dataset): The dataset to be split.
        n_training (int): Number of samples for the training set.
        n_validation (int): Number of samples for the validation set.

        Returns:
        torch.utils.data.dataset.random_split: A tuple containing training and validation datasets.
        """
        return random_split(trainset, [n_training, n_validation])

    n_training = len(trainset) - n_validation  # Calculate the number of samples for the training set


    # Splitting randomly into the given ratio, which is 80:20
    trainset, evalset = do_split(trainset, n_training, n_validation)  # Splitting trainset into training and validation sets

    def make_split(testset, a):
        """
        Function to split the dataset into two parts.

        Args:
        testset (torch.utils.data.Dataset): The dataset to be split.
        a (int): Number of samples for the first part.

        Returns:
        torch.utils.data.dataset.random_split: A tuple containing the two parts of the dataset.
        """
        return random_split(testset, [len(testset), a])

    testset, testset2 = make_split(testset, 0)  # Splitting testset into two parts


    #batch size of 32 is giving good performance, so we are taking all the samples in the batch of 32
    batch_size = 32
    #below is the utility function for testing
    def u2(x):
        return x>0

    shuffle=True
    trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle)
    #shuffing the data so that we always get the random set

    evalloader = torch.utils.data.DataLoader(evalset, batch_size, shuffle)
    testloader = torch.utils.data.DataLoader(testset, batch_size, shuffle)

    #creating an object of the Deep_Image_Classifier class
    net = Deep_Image_Classifier().to(device)

    # Setting the learning rate to 1e-4
    lr = 1e-4  # Learning rate

    # Defining the loss function
    loss_fn = nn.CrossEntropyLoss()

    # Getting the parameters of the neural network
    paramet = net.parameters()

    # Function for printing epoch and batch information
    def funct_print(epoch, i):
        """
        Function to print epoch and batch information.

        Args:
        epoch (int): Current epoch number.
        i (int): Current batch number.
        """
        print("epoch ", epoch, "batch ", i)

    # Defining the optimizer with Adamax optimizer and specified learning rate
    opt = optim.Adamax(paramet, lr)


    epoch=0
    while epoch < (maximum_training_epochs):
      #list created for storing loss in each epoch
        epoch_loss_array = []
        z=0
        for i, data in enumerate(trainloader, z):
          #printing for every 50th item
            if i%50==0:
                funct_print(epoch,i)

            #extracting the inputs and labels from the data
            inputs_data, labels_data = data  # Unpack data into inputs and labels

            def getinputLabels(self, labels):
                """
                Function to get input labels.

                Args:
                labels (torch.Tensor): Input labels.

                Returns:
                torch.Tensor: Input labels.
                """
                return labels

            inputs_data, labels_data = inputs_data.to(device), labels_data.to(device)  # Move inputs and labels to device


            # Applying optimizer to it
            def get_epoch_out(inputs_data):
                """
                Function to get the output of the neural network for an epoch.

                Args:
                inputs_data (torch.Tensor): Input data to the neural network.

                Returns:
                torch.Tensor: Output of the neural network.
                """
                return net(inputs_data)

            opt.zero_grad()  # Zero the gradients before backward pass

            def cal_loss(epoch_outputs, labels_data):
                """
                Function to calculate the loss.

                Args:
                epoch_outputs (torch.Tensor): Output of the neural network for an epoch.
                labels_data (torch.Tensor): True labels for the input data.

                Returns:
                torch.Tensor: Loss value.
                """
                return loss_fn(epoch_outputs, labels_data)


            #passing the input to the model for classication
            def update():
                """
                Function to perform one update step of the neural network parameters.

                """
                epoch_outputs = get_epoch_out(inputs_data)  # Get the output of the neural network for the current epoch

                loss_calculate = cal_loss(epoch_outputs, labels_data)  # Calculate the loss
                # Backward propagation
                loss_calculate.backward()  # Compute gradients
                opt.step()  # Update weights
                epoch_loss_array.append(loss_calculate.item())  # Append the loss value to the loss array

            update()  # Call the update function


        #calculating the train loss
        loss_train=sum(epoch_loss_array)/len(epoch_loss_array)
        train_acc, evaluating_loss = calculateAccAndError(trainloader, net, loss_fn)  # Calculate accuracy and loss for the training set
        def updateparas(self, parameters):
            """
            Function to update parameters.

            Args:
            parameters (any): Parameters to be updated.

            Returns:
            any: Updated parameters.
            """
            return parameters

        eval_acc, loss_eval = calculateAccAndError(evalloader, net, loss_fn)  # Calculate accuracy and loss for the validation set

        # Printing all the accuracy and loss
        print(f'train loss:- {loss_train} train acc:- {train_acc} val loss:- {loss_eval} val acc:- {eval_acc} ')

        # Logging accuracy and loss to Weights & Biases
        wandb.log({'train loss': loss_train, 'train acc': train_acc, 'val loss': loss_eval, 'val acc': eval_acc})

        epoch += 1  # Increment epoch count


#defining my train function which is used for initializing parameters selected in the sweep and then calling the main function with the selected parameters
def train(args):
    """
    Function to train the model.

    Args:
    args: Arguments containing various parameters.

    """
    wandb.init()  # Initialize Weights & Biases
    config = wandb.config  # Get configuration parameters from Weights & Biases
    # Set the name of the run based on the selected parameters
    wandb.run.name = 'maximum_training_epochs' + str(args.maximum_training_epochs) + '_conv_activation_' + args.conv_activation + '_enable_data_aug_' + str(args.enable_data_aug) + '_use_batch_normalization_' + str(args.use_batch_normalization) + '_fully_connected_activation_' + str(args.fully_connected_activation) + '_dropout_probability_' + str(args.dropout_probability) + '_hidden_layer_size_' + str(args.hidden_layer_size) + '_strideLength_' + str(args.strideLength)
    # Call the main function with the selected input
    main(args.num_filter, args.kernel_dimensions, args.conv_activation, args.enable_data_aug, args.use_batch_normalization, args.dropout_probability, args.hidden_layer_size, args.fully_connected_activation, args.strideLength, args.maximum_training_epochs)
    wandb.finish()  # Finish logging to Weights & Biases



#now defining main for command line argument
if __name__ == "__main__":
    # Ask for the WandB key
    wandb_key = input("Enter your WandB API key: ")
    # Login to WandB
    wandb.login(key=wandb_key)

    # Parse command-line arguments
    parser = argparse.ArgumentParser(description="Train a model with specified parameters or perform hyperparameter tuning using wandb sweeps")
    # Add command-line arguments
    # Initialize the argument parser with a description
    parser = argparse.ArgumentParser(description="Stores all the hyperparameters for the model.")

    # Define command-line arguments
    parser.add_argument("-wp", "--wandb_project", help="Project name used to track experiments in Weights & Biases dashboard", type=str, default="Deep_Learning_A2")  # Project name for Weights & Biases
    parser.add_argument("-we", "--wandb_entity", help="Wandb Entity used to track experiments in the Weights & Biases dashboard", type=str, default="cs23m030")  # Wandb Entity
    parser.add_argument("-lg", "--logger", help="Log to wandb or not", action="store_true")  # Option to log to Weights & Biases
    parser.add_argument("--num_filter", "-nf", nargs="+", type=int, default=[64, 64, 64, 64, 64], help="List of numbers of filters in the CNN layers")  # Number of filters in each CNN layer
    parser.add_argument("--kernel_dimensions", "-kd", nargs="+", type=int, default=[3, 3, 3, 3, 3], help="List of dimensions of the kernels in the CNN layers")  # Kernel dimensions for each CNN layer
    parser.add_argument("--conv_activation", "-ca", type=str, default="mish", help="Activation function for the convolutional layers")  # Activation function for convolutional layers
    parser.add_argument("--enable_data_aug", "-da", action="store_true", help="Enable data augmentation")  # Option to enable data augmentation
    parser.add_argument("--use_batch_normalization", "-bn", action="store_true", help="Use batch normalization")  # Option to use batch normalization
    parser.add_argument("--dropout_probability", "-dp", type=float, default=0.1, help="Dropout probability for fully connected layers")  # Dropout probability for fully connected layers
    parser.add_argument("--hidden_layer_size", "-hs", type=int, default=256, help="Size of the hidden layer in the fully connected layers")  # Size of the hidden layer
    parser.add_argument("--fully_connected_activation", "-fa", type=str, default="mish", help="Activation function for the fully connected layers")  # Activation function for fully connected layers
    parser.add_argument("--strideLength", "-sl", type=int, default=2, help="Stride length for max pooling")  # Stride length for max pooling
    parser.add_argument("--maximum_training_epochs", "-me", type=int, default=10, help="Maximum number of training epochs")  # Maximum number of training epochs
    parser.add_argument("--sweep", action="store_true", help="Perform hyperparameter tuning using wandb sweeps")  # Option to perform hyperparameter tuning

    args = parser.parse_args()  # Parse the command-line arguments

    # Perform sweep if --sweep argument is provided
    if args.sweep:
      # Define sweep configuration
      sweep_config = {
          "name": "sweep1",  # Sweep name
          "method": "bayes",  # Bayesian optimization method
          "metric": {
              "name": "val acc",  # Metric to optimize
              "goal": "maximize"  # Goal of optimization
          },
          "parameters": {
              "fully_connected_activation": {
                  "values": ['mish', 'relu', 'silu', 'gelu']  # Possible values for fully connected activation
              },
              "strideLength": {
                  "values": [2, 3, 5]  # Possible values for stride length
              },
              "enable_data_aug": {
                  "values": [True, False]  # Possible values for data augmentation
              },
              "num_filter": {
                  "values": [[32, 32, 32, 32, 32], [64, 64, 64, 64, 64], [512, 256, 128, 64, 32], [32, 64, 128, 256, 512]]  # Possible values for number of filters
              },
              "conv_activation": {
                  "values": ['mish', 'relu', 'silu', 'gelu']  # Possible values for convolutional activation
              },
              "kernel_dimensions": {
                  "values": [[11, 9, 7, 5, 3], [5, 5, 5, 5, 5], [3, 3, 3, 3, 3]]  # Possible values for kernel dimensions
              },
              "use_batch_normalization": {
                  "values": [False, True]  # Possible values for batch normalization
              },
              "maximum_training_epochs": {
                  "values": [5, 8, 10]  # Possible values for maximum training epochs
              },
              "dropout_probability": {
                  "values": [0.3, 0.2, 0.1]  # Possible values for dropout probability
              },
              "hidden_layer_size": {
                  "values": [128, 256, 512]  # Possible values for hidden layer size
              }
          }
      }

      # Setup sweep
      sweep_id = wandb.sweep(sweep=sweep_config, project="ASSIGN_2_DL_testing")  # Create a sweep
      # Run agent
      wandb.agent(sweep_id, train, count=1)  # Run the agent with the specified sweep ID and function, once

    else:
        # Call the train function with command-line arguments
        train(args)
