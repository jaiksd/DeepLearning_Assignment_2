# -*- coding: utf-8 -*-
"""Train_part_ B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12pWZp9HzigQEXTdWMw3uFiDq71XuiR__
"""

# Importing all the necessary libraries
import torch  # Import PyTorch library
import numpy as np  # Import numpy for numerical computations
import torchvision  # Import torchvision library for computer vision tasks
import torch.nn.functional as F  # Import functional module from PyTorch for functional operations
import matplotlib.pyplot as plt  # Import matplotlib for plotting
from torch.utils.data import random_split  # Import random_split for splitting datasets
import tqdm  # Import tqdm for progress bar visualization
# !pip install --upgrade wandb  # Install or upgrade wandb library
import wandb  # Import wandb for experiment tracking
import argparse  # Import argparse for parsing command-line arguments
import torch.optim as optim  # Import optimization module from PyTorch
import torch.nn as nn  # Import neural network module from PyTorch
from torchvision.datasets import ImageFolder  # Import ImageFolder dataset from torchvision
import torchvision.transforms as transforms



#logging into wandb
# !wandb login fbf80504ccef17f5f3b05723be7ea4caff805164

#installing wget to download the dataset from the link
# !pip install wget
import wget
# wget.download('https://storage.googleapis.com/wandb_datasets/nature_12K.zip')
# !unzip /content/nature_12K.zip

torch.cuda.device_count()  # Check the number of available CUDA devices
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # Set the device to CUDA if available, otherwise to CPU
print(device)  # Print the selected device

def dataset_creation(data_augmentation=True):
  """
  Function to create dataset transforms.

  Args:
  data_augmentation (bool): Flag to enable data augmentation.

  Returns:
  tuple: Tuple containing train and validation transforms.
  """
  resize = 224  # Resize the images to 224x224
  degrees = 30  # Rotation degree range for data augmentation
  brightness = 0.4  # Brightness factor range for data augmentation
  contrast = 0.4  # Contrast factor range for data augmentation
  saturation = 0.4  # Saturation factor range for data augmentation
  hue = 0.1  # Hue factor range for data augmentation
  translate = (0.1, 0.1)  # Translation range for data augmentation
  scale = (0.8, 1.2)  # Scale range for data augmentation
  shear = 10  # Shear angle range for data augmentation
  mean = [0.485, 0.456, 0.406]  # Mean values for image normalization
  variance = [0.229, 0.224, 0.225]  # Variance values for image normalization

  if data_augmentation:
    # Data augmentation transforms
    train_transforms = transforms.Compose([
        transforms.Resize((resize, resize)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(degrees),
        transforms.ColorJitter(brightness, contrast, saturation, hue),
        transforms.RandomAffine(degrees, translate, scale, shear),
        transforms.ToTensor(),
        transforms.Normalize(mean, variance)
    ])
  else:
    # No data augmentation transforms
    train_transforms = transforms.Compose([
        transforms.Resize((resize, resize)),
        transforms.ToTensor(),
        transforms.Normalize(mean, variance)
    ])

  # Validation transforms
  val_transforms = transforms.Compose([
      transforms.Resize((resize, resize)),
      transforms.ToTensor(),
      transforms.Normalize(mean, variance)
  ])

  # Create training and validation datasets

  def ret_train(train_dataset):
      """
      Calculate the size of the training dataset.

      Args:
      train_dataset: The training dataset.

      Returns:
      int: Size of the training dataset.
      """
      return int(0.8 * len(train_dataset))  # 80% of the total dataset for training

  def ret_val_size(train_dataset, train_size):
      """
      Calculate the size of the validation dataset.

      Args:
      train_dataset: The training dataset.
      train_size (int): Size of the training dataset.

      Returns:
      int: Size of the validation dataset.
      """
      return len(train_dataset) - train_size  # Remaining data for validation



  train_dataset = ImageFolder('/content/inaturalist_12K/train', transform=train_transforms)  # Create training dataset
  train_size = ret_train(train_dataset)  # Calculate size of training dataset
  val_size = ret_val_size(train_dataset, train_size)  # Calculate size of validation dataset

  def ret_split(train_dataset, train_size, val_size):
      """
      Split the dataset into training and validation sets.

      Args:
      train_dataset: The training dataset.
      train_size (int): Size of the training dataset.
      val_size (int): Size of the validation dataset.

      Returns:
      tuple: Tuple containing training and validation datasets.
      """
      return random_split(train_dataset, [train_size, val_size])  # Random split the dataset

  train_dataset, val_dataset = ret_split(train_dataset, train_size, val_size)  # Split training dataset into training and validation sets
  test_dataset = ImageFolder('/content/inaturalist_12K/val', transform=val_transforms)  # Create test dataset
# Create training and validation data loaders
  BATCH_SIZE = 32  # Set batch size for data loaders
  batch_size = BATCH_SIZE  # Alias for batch size
  shuffle = True  # Flag to shuffle the data during training
  num_workers = 2  # Number of subprocesses to use for data loading

  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2)  # Create training data loader
  shuffle = False  # Disable shuffling for validation data loader
  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False, num_workers=2)  # Create validation data loader

  return train_loader, val_loader  # Return training and validation data loaders

train_loader, val_loader = dataset_creation()  # Create training and validation data loaders using dataset_creation function

# Installing timm
# !pip install timm

import timm  # Import timm for using pre-trained models

def ret_features(num_features, a):
    """
    Return a fully connected layer with specified input and output features.

    Args:
    num_features (int): Number of input features.
    a (int): Number of output features.

    Returns:
    nn.Linear: Fully connected layer.
    """
    return nn.Linear(num_features, a)

def efficientnet(freeze_percent):
    """
    Create an EfficientNet model with specified freeze percentage and number of output features.

    Args:
    freeze_percent (float): Percentage of layers to freeze.

    Returns:
    torch.nn.Module: EfficientNet model with modified classifier.
    """
    # Load the EfficientNetV2 model with pre-trained weights
    model = timm.create_model('tf_efficientnetv2_s', pretrained=True)
    
    def ret_sum(model):
        return sum(1 for _ in model.parameters())
    
    # Freeze a percentage of layers
    count_total = ret_sum(model)
    count = 0
    
    def ret_cal(freeze_percent, count_total):
        return int(freeze_percent * count_total)
    
    for param in model.parameters():
        if count < ret_cal(freeze_percent, count_total):
            param.requires_grad = False
            count += 1
        else:
            break

    # Replace the last layer with a new one for the specified number of classes
    num_features = model.classifier.in_features
    feature_count = 10
    model.classifier = ret_features(num_features, feature_count)

    return model

def ret_sweep(args):
    sweep_config = {
    'name': 'sweep1',
    'method': 'bayes', #grid, random,bayes
    'metric': {
      'name': 'val_accuracy',
      'goal': 'maximize'
    },
    'parameters': {

        'learning_rate':{
            'values':[0.0001,0.0003]
        },
        'beta1':{
            'values':[0.9,0.93,0.95]
        },
        'freeze_percent': {
            'values': [0.25,0.5,0.75]
        }
    }
    }

    return wandb.sweep(sweep_config, entity=args.wandb_entity, project=args.wandb_project)
# sweep_id = ret_sweep(args)

def main(freeze_percent=0.25,learning_rate=0.0001,beta1=0.9):
  def train_2():
      train(10,3,2)

  def create_new_run():
      train(10,4,2)
  wandb.init()
  config = wandb.config
  # wandb.init(project='ASSIGN_2_DL_testing', entity='cs23m030',config=config_defaults)
  def ret_run_name():
      return 'fp:'+ str(args.freeze_percent)+' ;lr:'+str(args.learning_rate)+ ' ;beta1:'+str(args.beta1)
  # wandb.run.name = 'fp:'+ str(args.freeze_percent)+' ;lr:'+str(args.learning_rate)+ ' ;beta1:'+str(args.beta1)

  
  def ret_freeze_percent():
      return freeze_percent
  def ret_lr():
      return learning_rate
  freeze_percent = ret_freeze_percent()
  learning_rate = ret_lr()
  def ret_beta():
      return beta1
  beta1 = ret_beta()
  # Model training here
  #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model = efficientnet(freeze_percent).to(device)
  def ret_loss():
      return nn.CrossEntropyLoss()
  train_loader,val_loader=dataset_creation()

  criterion = ret_loss()
  def ret_optim():
      return optim.Adam(model.parameters(),lr=learning_rate, betas=(beta1,0.9999))
  optimizer = ret_optim()#using adam
  num_epochs = 10
  epoch=0
  while epoch < (num_epochs):
          # training the model
          def model_training():
              model.train()
          model_training()

          running_loss = 0.0
          running_corrects = 0
          for inputs, labels in train_loader:
              inputs, labels = inputs.to(device), labels.to(device)
              def ret_outputs(inputs):
                  return model(inputs)
              # Zero the parameter gradients
              optimizer.zero_grad()
              def ret_pred(outputs):
                  return torch.max(outputs, 1)

              # Forward pass
              outputs = ret_outputs(inputs)
              _, preds = ret_pred(outputs)
              loss = criterion(outputs, labels)

              # Backward pass and optimize
              loss.backward()
              optimizer.step()

              loss_calc=loss.item() * inputs.size(0)
              # Statistics
              running_loss += loss_calc
              correct_calc = torch.sum(preds == labels.data)
              running_corrects += correct_calc

          def calc_el(running_loss,train_loader):
              return running_loss / len(train_loader.dataset)
          train_epoch_loss = calc_el(running_loss, train_loader)
          def epoch_acc(running_corrects, train_loader):
              return (running_corrects.double() / len(train_loader.dataset))*100
          train_epoch_acc = epoch_acc(running_corrects, train_loader)


          # Evaluate on validation set
          model.eval()
          def ret_run_loss():
              return 0.0

          def ret_cor():
              return 0

          running_loss = ret_run_loss()
          running_corrects = ret_cor()
          with torch.no_grad():
              for inputs, labels in val_loader:
                  a_in = ret_run_loss()
                  inputs, labels = inputs.to(device), labels.to(device)

                  def ret_outputs(inputs):
                      return model(inputs)
                  # Forward pass
                  outputs = ret_outputs(inputs)
                  def ret_pred(outputs):
                      return torch.max(outputs, 1)
                  _, preds = ret_pred(outputs)
                  loss = criterion(outputs, labels)

                  # Statistics
                  cal_run_loss = loss.item() * inputs.size(0)
                  running_loss += cal_run_loss
                  cal_run_cor=torch.sum(preds == labels.data)
                  running_corrects += cal_run_cor
          def ret_v(running_loss,val_loader):
              return running_loss / len(val_loader.dataset)

          val_epoch_loss = ret_v(running_loss,val_loader)
          def ret_acc(running_corrects,val_loader):
              return (running_corrects.double() / len(val_loader.dataset))*100
          val_epoch_acc = ret_acc(running_corrects , val_loader)

          def print_val():
              print(f"Epoch {epoch+1}/{num_epochs}--> Training_Loss:{train_epoch_loss:.2f}; Train_Accuracy:{train_epoch_acc:.2f}; Validation_Loss:{val_epoch_loss:.2f}; Val_Accuracy:{val_epoch_acc:.2f}")
          print_val()
          def logging_wandb():
              wandb.log({"train_loss":train_epoch_loss,"train_accuracy": train_epoch_acc,"val_loss":val_epoch_loss,"val_accuracy":val_epoch_acc},)
          logging_wandb()
          if epoch==num_epochs-1:
            torch.cuda.empty_cache()
          epoch+=1

def training_for_sweep(args):
    """
    Function for training with sweep parameters.

    Args:
    args: Arguments parsed from the command line.
    """
    # Setting the default values for sweep
    config_defaults = {
        'beta1': args.beta1,
        'learning_rate': args.learning_rate,
        'freeze_percent': args.freeze_percent,
    }

    wandb.init()  # Initialize wandb for experiment tracking
    config = wandb.config  # Get wandb configuration

    # Taking various parameters from the sweep
    wandb.run.name = 'fp:' + str(args.freeze_percent) + ' ;lr:' + str(args.learning_rate) + ' ;beta1:' + str(
        args.beta1)

    # Calling the main function with the selected input
    main(args.freeze_percent, args.learning_rate, args.beta1)

    # Creating a new wandb run
    wandb.finish()


if __name__ == "__main__":
    # Prompt the user to enter the Wandb API key
    wandb_key = input("Enter your Wandb API key: ")
    # Set Wandb API key
    wandb.login(key=wandb_key)
    # Parse command-line arguments

    # Create an ArgumentParser object with a description
    parser = argparse.ArgumentParser(description="Stores all the hyperparameters for the model.")
    # Add argument for specifying Wandb project name
    parser.add_argument("-wp", "--wandb_project", help="Project name used to track experiments in Weights & Biases dashboard", type=str, default="ASSIGN_2_DL")
    # Add argument for specifying Wandb entity
    parser.add_argument("-we", "--wandb_entity", help="Wandb Entity used to track experiments in the Weights & Biases dashboard", type=str, default="cs23m030")
    # Add argument for enabling logging to Wandb
    parser.add_argument("-lg", "--logger", help="Log to wandb or not", action="store_true")
    # Add argument for specifying freeze percent
    parser.add_argument("-fp", "--freeze_percent", type=float, default=0.25, help='Freeze percent values')
    # Add argument for specifying learning rate
    parser.add_argument("-lr", "--learning_rate", type=float, default=0.0001, help='Learning rate values')
    # Add argument for specifying beta1 value
    parser.add_argument("-b1", "--beta1", type=float, default=0.9, help='Beta1 values')
    # Add argument for specifying the number of runs for the sweep
    parser.add_argument("-c", "--count", type=int, default=1, help='Number of runs for the sweep')
    # Add argument for enabling hyperparameter tuning using sweeps
    parser.add_argument("--sweep", action="store_true", help="Perform hyperparameter tuning using wandb sweeps")
    # Parse the arguments
    args = parser.parse_args()

    # Check if hyperparameter tuning using sweeps is enabled
    if args.sweep:
        # Define the configuration for the sweep
        sweep_config = {
            'name': 'sweep1',  # Name of the sweep
            'method': 'bayes',  # Method for hyperparameter optimization (grid, random, bayes)
            'metric': {  # Metric to optimize during the sweep
                'name': 'val_accuracy',  # Name of the metric
                'goal': 'maximize'  # Goal of the optimization (maximize or minimize)
            },
            'parameters': {  # Hyperparameters to tune
                'learning_rate': {  # Learning rate values
                    'values': [0.0001, 0.0003]  # List of learning rate values
                },
                'beta1': {  # Beta1 values
                    'values': [0.9, 0.93, 0.95]  # List of beta1 values
                },
                'freeze_percent': {  # Freeze percent values
                    'values': [0.25, 0.5, 0.75]  # List of freeze percent values
                }
            }
        }
        # Create a sweep and get the sweep ID
        sweep_id = wandb.sweep(sweep=sweep_config, project="ASSIGN_2_DL_testing")
        # Run the agent for the sweep
        wandb.agent(sweep_id, function=training_for_sweep, count=1)
    # If hyperparameter tuning using sweeps is not enabled, run training for a single set of hyperparameters
    else:
        training_for_sweep(args)
